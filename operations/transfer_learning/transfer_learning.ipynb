{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color:red\">SuAVE Transfer Learning Model</span></h1>\n",
    "\n",
    "<h2> WORK IN PROGRESS </h2>\n",
    "Author: Shuyuan Wang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Disable autoscroll and retrieve survey parameters from the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "function getQueryStringValue (key)\n",
       "{  \n",
       "    return unescape(window.location.search.replace(new RegExp(\"^(?:.*[&\\\\?]\" + escape(key).replace(/[\\.\\+\\*]/g, \"\\\\$&\") + \"(?:\\\\=([^&]*))?)?.*$\", \"i\"), \"$1\"));\n",
       "}\n",
       "IPython.notebook.kernel.execute(\"survey_url='\".concat(getQueryStringValue(\"surveyurl\")).concat(\"'\"));\n",
       "IPython.notebook.kernel.execute(\"views='\".concat(getQueryStringValue(\"views\")).concat(\"'\"));\n",
       "IPython.notebook.kernel.execute(\"view='\".concat(getQueryStringValue(\"view\")).concat(\"'\"));\n",
       "IPython.notebook.kernel.execute(\"user='\".concat(getQueryStringValue(\"user\")).concat(\"'\"));\n",
       "IPython.notebook.kernel.execute(\"csv_file='\".concat(getQueryStringValue(\"csv\")).concat(\"'\")); \n",
       "IPython.notebook.kernel.execute(\"dzc_file='\".concat(getQueryStringValue(\"dzc\")).concat(\"'\")); \n",
       "IPython.notebook.kernel.execute(\"params='\".concat(getQueryStringValue(\"params\")).concat(\"'\")); \n",
       "IPython.notebook.kernel.execute(\"active_object='\".concat(getQueryStringValue(\"activeobject\")).concat(\"'\")); \n",
       "IPython.notebook.kernel.execute(\"full_notebook_url='\" + window.location + \"'\"); \n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "function getQueryStringValue (key)\n",
    "{  \n",
    "    return unescape(window.location.search.replace(new RegExp(\"^(?:.*[&\\\\?]\" + escape(key).replace(/[\\.\\+\\*]/g, \"\\\\$&\") + \"(?:\\\\=([^&]*))?)?.*$\", \"i\"), \"$1\"));\n",
    "}\n",
    "IPython.notebook.kernel.execute(\"survey_url='\".concat(getQueryStringValue(\"surveyurl\")).concat(\"'\"));\n",
    "IPython.notebook.kernel.execute(\"views='\".concat(getQueryStringValue(\"views\")).concat(\"'\"));\n",
    "IPython.notebook.kernel.execute(\"view='\".concat(getQueryStringValue(\"view\")).concat(\"'\"));\n",
    "IPython.notebook.kernel.execute(\"user='\".concat(getQueryStringValue(\"user\")).concat(\"'\"));\n",
    "IPython.notebook.kernel.execute(\"csv_file='\".concat(getQueryStringValue(\"csv\")).concat(\"'\")); \n",
    "IPython.notebook.kernel.execute(\"dzc_file='\".concat(getQueryStringValue(\"dzc\")).concat(\"'\")); \n",
    "IPython.notebook.kernel.execute(\"params='\".concat(getQueryStringValue(\"params\")).concat(\"'\")); \n",
    "IPython.notebook.kernel.execute(\"active_object='\".concat(getQueryStringValue(\"activeobject\")).concat(\"'\")); \n",
    "IPython.notebook.kernel.execute(\"full_notebook_url='\" + window.location + \"'\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import all packages (this might take a few seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import widget functionality\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Markdown, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n",
    "# import the necessary packages\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from keras.utils import to_categorical\n",
    "from imutils import paths\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras import backend as K\n",
    "# import local lenet.py file describing the LeNet implementation with RELU activation functions\n",
    "#from lenet import LeNet\n",
    "\n",
    "# More imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# import the necessary packages for SVM predictor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import imutils\n",
    "\n",
    "\n",
    "# copy from sample\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.applications import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initializing the number of times the model will loop and its batch size for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "epochCount = OrderedDict()\n",
    "epochCount['25 Iterations'] = 25\n",
    "epochCount['50 Iterations'] = 50\n",
    "epochCount['75 Iterations'] = 75\n",
    "\n",
    "def f(epoch_count):\n",
    "    return epoch_count\n",
    "\n",
    "epochNum = interact(f, epoch_count=epochCount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "batchS = OrderedDict()\n",
    "batchS['32 Batch Size'] = 32\n",
    "batchS['64 Batch Size'] = 64\n",
    "batchS['128 Batch Size'] = 128\n",
    "\n",
    "def f(batch_size):\n",
    "    return batch_size\n",
    "\n",
    "batchNum = interact(f, batch_size=batchS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the number of epochs to train for, init learning rate and batch size\n",
    "EPOCHS = epochNum.widget.result\n",
    "INIT_LR = 1e-3\n",
    "BS = batchNum.widget.result\n",
    "# init the image suffix, yset, and image list\n",
    "suffix = '.png'\n",
    "img_list = []\n",
    "yset = []\n",
    "# create labels list and 2 dicts for 2 way mapping\n",
    "labels = []\n",
    "# key = label value = number\n",
    "label_yval = dict()\n",
    "# key = number value = label\n",
    "yval_label = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the data and choose the column name to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
   ],
   "source": [
    "# use csv file to grab images/labels\n",
    "absolutePath = \"/home/jovyan/temp_csvs/\"\n",
    "# read the csv file\n",
    "file = open(absolutePath + csv_file, encoding=\"latin-1\")\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "#generate image path\n",
    "localdzc = dzc_file.replace(\"https://maxim.ucsd.edu/dzgen/lib-staging-uploads\",\"/lib-nfs/dzgen\")\n",
    "full_images_location = localdzc.replace(\"/content.dzc\",\"/full_images/\")\n",
    "\n",
    "\n",
    "# Choose column of label for prediction\n",
    "toPredict = list(df.columns.values)\n",
    "\n",
    "pred_menu = OrderedDict()\n",
    "for i in range(0, len(toPredict)):\n",
    "    pred_menu[toPredict[i]] = toPredict[i]\n",
    "\n",
    "def f(predictions_menu):\n",
    "    return predictions_menu\n",
    "# choose which label for predictions\n",
    "out2 = interact(f, predictions_menu=pred_menu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Select pixel resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acad3d599298422d98084089cc30b252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=60, description='Size, pixels:', max=300, min=20, step=10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = widgets.IntSlider(value=60,min=20,max=300,step=10,description='Size, pixels:')\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "printmd(\"<h2><span style='color:red'>Verify model parameters:</span></h2>\")\n",
    "printmd(\"<b>Variable to predict:  \" +out2.widget.result + \"</b>\")\n",
    "printmd(\"<b>Pixel resolution:  \" +str(a.value) + \"</b>\")\n",
    "printmd(\"<b>Path to images:  \" +full_images_location + \"</b>\")\n",
    "printmd(\"<b>Number of epochs:  \" + str(EPOCHS) + \"</b>\")\n",
    "printmd(\"<b>Batch size:  \" +str(BS) + \"</b>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Grab the images and configure them for predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This might take a little while depending on the size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07d79273ce34d0ba0556f0a0abc6696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='0% images loaded')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im_dimension = a.value\n",
    "# grab chosen column names\n",
    "nameCol = df['#img']\n",
    "predCol = df[out2.widget.result]\n",
    "\n",
    "def matchImage(curr_image_array, image_list):\n",
    "    for i in range(0, len(image_list)):\n",
    "        if (np.array_equal(curr_image_array, image_list[i])):\n",
    "            return i\n",
    "    \n",
    "labels = []\n",
    "# add all fabric columns to the y set\n",
    "for i in range (0,len(predCol)):\n",
    "    labels.append(predCol[i])\n",
    "\n",
    "# grab all unique labels\n",
    "uni_labels = set(labels)\n",
    "uni_labels = list(uni_labels)\n",
    "\n",
    "# assign each label a dict key number\n",
    "for i in range(0,len(uni_labels)):\n",
    "    yval_label[i] = uni_labels[i]\n",
    "    label_yval[uni_labels[i]] = i\n",
    "\n",
    "\n",
    "yset = []    \n",
    "# create list of keys associated with their labels\n",
    "for i in range (0, len(labels)):\n",
    "    yset.append(label_yval[labels[i]])\n",
    "\n",
    "img_list = []\n",
    "file_lst=[]\n",
    "\n",
    "counter = 0\n",
    "im_count = widgets.Label(value=\"0% images loaded\")\n",
    "display(im_count)\n",
    "numfiles = len(nameCol)\n",
    "\n",
    "# gather images from path created from file names in csv file\n",
    "for i in range (0,len(nameCol)):\n",
    "    base_filename = nameCol[i]\n",
    "\n",
    "    fileName = os.path.join(full_images_location, base_filename + suffix)\n",
    "    \n",
    "    file_lst.append(fileName)\n",
    "    \n",
    "    response = requests.get(fileName)\n",
    "    if response.status_code == 200:\n",
    "        # Read the image data into a NumPy array\n",
    "        image_data = np.frombuffer(response.content, np.uint8)\n",
    "        \n",
    "        # Decode the image using OpenCV\n",
    "        im = cv2.imdecode(image_data, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        # Resize the image\n",
    "        im_dimension = 256  # Adjust the dimension as needed\n",
    "        im = cv2.resize(im, (im_dimension, im_dimension))\n",
    "        \n",
    "        # Convert the image to a Keras-compatible format\n",
    "        im = img_to_array(im)\n",
    "        \n",
    "        # Append the processed image to the list\n",
    "        img_list.append(im)\n",
    "    else:\n",
    "        print(f\"Failed to fetch image from URL: {url}\")\n",
    "    \n",
    "    counter += 1\n",
    "    im_count.value = str(int(counter / numfiles * 100)) + \"% images loaded\"\n",
    "\n",
    "# Shuffle the data\n",
    "p = np.random.permutation(len(yset))\n",
    "\n",
    "test_train_dict = {}\n",
    "test_train_list = []\n",
    "\n",
    "# Relable for splitting sets\n",
    "Y = []\n",
    "X = []\n",
    "for i in range(0,len(yset)):\n",
    "    Y.append(yset[p[i]])\n",
    "    X.append(file_lst[p[i]])\n",
    "    \n",
    "# split the test and training set 75:25\n",
    "split = int(len(X)*(.75))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf=pd.DataFrame()\n",
    "ndf['id']=file_lst\n",
    "ndf['label']=list(map(str, yset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 validated image filenames belonging to 0 classes.\n",
      "Found 0 validated image filenames belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1/255., validation_split=0.25)\n",
    " \n",
    "train_generator = datagen.flow_from_dataframe(dataframe=ndf, \n",
    "                                             x_col='id',\n",
    "                                             y_col='label',\n",
    "                                             target_size=(224,224),\n",
    "                                              subset='training',\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary',\n",
    "                                              \n",
    "                                                 shuffle=True)\n",
    " \n",
    "validation_generator = datagen.flow_from_dataframe(dataframe=ndf, \n",
    "                                             x_col='id',\n",
    "                                             y_col='label',\n",
    "                                             target_size=(224,224),\n",
    "                                                subset='validation',\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary',\n",
    "                                                 shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 23:29:13.511353: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-09-13 23:29:13.511397: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-09-13 23:29:13.511425: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-jkaminsky-40ucsd-2eedu): /proc/driver/nvidia/version does not exist\n",
      "2023-09-13 23:29:13.511738: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model=ResNet50(weights='imagenet',include_top=False)\n",
    "\n",
    "newOutput=base_model.output\n",
    "newOutput=GlobalAveragePooling2D()(newOutput)\n",
    "newOutput=Dense(1024,activation='relu')(newOutput) \n",
    "newOutput=Dense(1024,activation='relu')(newOutput) \n",
    "newOutput=Dense(512,activation='relu')(newOutput)\n",
    "\n",
    "preds=Dense(2,activation='softmax')(newOutput)\n",
    "\n",
    "\n",
    "model=Model(inputs=base_model.input,outputs=preds)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer = 'Adam',metrics=['accuracy'])\n",
    "\n",
    "train_steps = train_generator.n//train_generator.batch_size\n",
    "validation_steps = validation_generator.n//validation_generator.batch_size\n",
    "\n",
    "#history = model.fit_generator(train_generator,steps_per_epoch=train_steps, epochs=5,\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Asked to retrieve element 0, but the Sequence has length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:2604\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2592\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[1;32m   2593\u001b[0m \n\u001b[1;32m   2594\u001b[0m \u001b[38;5;124;03mDEPRECATED:\u001b[39;00m\n\u001b[1;32m   2595\u001b[0m \u001b[38;5;124;03m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[1;32m   2596\u001b[0m \u001b[38;5;124;03m  use this endpoint.\u001b[39;00m\n\u001b[1;32m   2597\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2598\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2599\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.fit_generator` is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2600\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2601\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2602\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   2603\u001b[0m )\n\u001b[0;32m-> 2604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2616\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2618\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2619\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/preprocessing/image.py:103\u001b[0m, in \u001b[0;36mIterator.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to retrieve element \u001b[39m\u001b[38;5;132;01m{idx}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut the Sequence \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas length \u001b[39m\u001b[38;5;132;01m{length}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx\u001b[38;5;241m=\u001b[39midx, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m    107\u001b[0m         )\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_batches_seen)\n",
      "\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,steps_per_epoch=train_steps, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Asked to retrieve element 0, but the Sequence has length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m,loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     17\u001b[0m step_size_train\u001b[38;5;241m=\u001b[39mtrain_generator\u001b[38;5;241m.\u001b[39mn\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mtrain_generator\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m---> 18\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:2604\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2592\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[1;32m   2593\u001b[0m \n\u001b[1;32m   2594\u001b[0m \u001b[38;5;124;03mDEPRECATED:\u001b[39;00m\n\u001b[1;32m   2595\u001b[0m \u001b[38;5;124;03m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[1;32m   2596\u001b[0m \u001b[38;5;124;03m  use this endpoint.\u001b[39;00m\n\u001b[1;32m   2597\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2598\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2599\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.fit_generator` is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2600\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2601\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2602\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   2603\u001b[0m )\n\u001b[0;32m-> 2604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2616\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2618\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2619\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/preprocessing/image.py:103\u001b[0m, in \u001b[0;36mIterator.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to retrieve element \u001b[39m\u001b[38;5;132;01m{idx}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut the Sequence \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas length \u001b[39m\u001b[38;5;132;01m{length}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx\u001b[38;5;241m=\u001b[39midx, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m    107\u001b[0m         )\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_batches_seen)\n",
      "\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
     ]
    }
   ],
   "source": [
    "base_model=ResNet50(weights='imagenet',include_top=False)\n",
    "\n",
    "newOutput=base_model.output\n",
    "newOutput=GlobalAveragePooling2D()(newOutput)\n",
    "newOutput=Dense(1024,activation='relu')(newOutput) \n",
    "newOutput=Dense(1024,activation='relu')(newOutput) \n",
    "newOutput=Dense(512,activation='relu')(newOutput)\n",
    "\n",
    "preds=Dense(2,activation='softmax')(newOutput)\n",
    "\n",
    "\n",
    "model=Model(inputs=base_model.input,outputs=preds)\n",
    "\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                   steps_per_epoch=step_size_train,\n",
    "                   epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_ext(fn):\n",
    "    return fn+\".png\"\n",
    "traindf=pd.read_csv(“./trainLabels.csv”,dtype=str)\n",
    "testdf=pd.read_csv(\"./sampleSubmission.csv\",dtype=str)\n",
    "traindf[\"id\"]=traindf[\"id\"].apply(append_ext)\n",
    "testdf[\"id\"]=testdf[\"id\"].apply(append_ext)\n",
    "datagen=ImageDataGenerator(rescale=1./255.,validation_split=0.25)\n",
    "\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "dataframe=traindf,\n",
    "directory=\"./train/\",\n",
    "x_col=\"id\",\n",
    "y_col=\"label\",\n",
    "subset=\"training\",\n",
    "batch_size=32,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"categorical\",\n",
    "target_size=(32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "p = np.random.permutation(len(yset))\n",
    "\n",
    "test_train_dict = {}\n",
    "test_train_list = []\n",
    "\n",
    "# Relable for splitting sets\n",
    "Y = []\n",
    "X = []\n",
    "for i in range(0,len(yset)):\n",
    "    Y.append(yset[p[i]])\n",
    "    X.append(img_list[p[i]])\n",
    "    \n",
    "# split the test and training set 75:25\n",
    "split = int(len(X)*(.75))\n",
    "\n",
    "## Original DO NOT DELETE\n",
    "#xtrain = X[:split]\n",
    "#xtest = X[split:]\n",
    "\n",
    "\n",
    "## Testing new \n",
    "xtrain = []\n",
    "xtest = []\n",
    "for i in range(0, len(X)):\n",
    "    \n",
    "    if (i < split):\n",
    "        curr_image_index = matchImage(X[i], img_list)\n",
    "        test_train_dict[nameCol[curr_image_index]] = \"train\"\n",
    "        xtrain.append(X[i])\n",
    "    else:\n",
    "        curr_image_index = matchImage(X[i], img_list)\n",
    "        test_train_dict[nameCol[curr_image_index]] = \"test\"\n",
    "        xtest.append(X[i])\n",
    "\n",
    "    \n",
    "    \n",
    "ytrain = Y[:split]\n",
    "ytest = Y[split:]\n",
    "\n",
    "for i in range(0, len(nameCol)):\n",
    "    #print(i)\n",
    "    #print(nameCol[i])\n",
    "    test_train_list.append(test_train_dict[nameCol[i]])\n",
    "\n",
    "\n",
    "# transform to arrays\n",
    "trainX = np.array(xtrain, dtype=\"float\")/255.0\n",
    "testX = np.array(xtest, dtype =\"float\")/255.0\n",
    "\n",
    "ytrain = np.array(ytrain)\n",
    "ytest = np.array(ytest)\n",
    "\n",
    "# parsed Y data containers\n",
    "trainY = []\n",
    "testY = []\n",
    "\n",
    "\n",
    "# convert labels from int to vectors\n",
    "trainY = np_utils.to_categorical(ytrain,len(uni_labels))\n",
    "testY = np_utils.to_categorical(ytest,len(uni_labels))\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
    "                        height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "                        horizontal_flip=True, fill_mode=\"nearest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.fromarray(np.uint8(cm.gist_earth(myarray)*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.fromarray(np.uint8(cm.gist_earth(myarray)*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myarray=trainX[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "\n",
    "model = LeNet.build(width=im_dimension, height=im_dimension, depth=3, classes=len(uni_labels))\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "printmd(\"<b><span style='color:red'>Images split into training and testing sets at 75:25</span></b>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "#train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "#train_generator=train_datagen.flow_from_directory(folder_path,\n",
    "                                                 #target_size=(224,224),\n",
    "                                                 #color_mode='rgb',\n",
    "                                                 #batch_size=32,\n",
    "                                                 #class_mode='categorical',\n",
    "                                                 #shuffle=True)\n",
    "# opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "#model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is relative to the size of the data set and the compute resources you use. May take from minutes to hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "# step_size_train=train_generator.n//train_generator.batch_size\n",
    "# model.fit_generator(generator=train_generator,\n",
    "                   #steps_per_epoch=step_size_train,\n",
    "                   #epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS),\n",
    "    validation_data=(testX, testY), steps_per_epoch=len(trainX) // BS,\n",
    "    epochs=EPOCHS, verbose=1)\n",
    "printmd(\"<b><span style='color:red'>Model generation complete</span></b>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Take the original data and predict based on the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape original input data images for predicting\n",
    "img_check = np.array(img_list, dtype =\"float\")/255.0\n",
    "\n",
    "predictionsMade = []\n",
    "preds = model.predict(img_check)\n",
    "prediction_confidence = []\n",
    "for i in range(0, len(preds)):\n",
    "    prediction_confidence.append(np.amax(preds[i]))   \n",
    "\n",
    "\n",
    "\n",
    "# Run all data through the prediction model that was created\n",
    "for i in range (0,len(img_check)):\n",
    "    predIndex = np.where(preds[i] == np.amax(preds[i]))\n",
    "    prediction = int(predIndex[0][0])\n",
    "    predictionsMade.append(prediction)\n",
    "\n",
    "print(prediction_confidence)    \n",
    "# Count how many correct predictions were made\n",
    "correct = 0\n",
    "for i in range (0,len(predictionsMade)):\n",
    "    if(predictionsMade[i] == yset[i]):\n",
    "        correct += 1 \n",
    "        \n",
    "printmd(\"<b><span style='color:red'>Accuracy: \" + str(correct/len(yset)) + \"</span></b>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save the model file under models/, and reload it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate model file and save\n",
    "modelName = user + \"_cnn_\" + out2.widget.result + \"_\" + str(epochNum.widget.result) + \"_\" + str(batchNum.widget.result) + \".h5\"\n",
    "modelPath = \"models/\"\n",
    "if not os.path.exists(modelPath):\n",
    "    os.makedirs(modelPath)\n",
    "model.save(os.path.join(modelPath, modelName))\n",
    "#Load model\n",
    "from keras.models import load_model\n",
    "model2 = load_model(os.path.join(modelPath, modelName))\n",
    "printmd(\"<b><span style='color:red'>Model saved</span></b>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Enter a new header for the prediction column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate back to original csv label names\n",
    "finalPred = []\n",
    "for i in range (0,len(predictionsMade)):\n",
    "    finalPred.append(yval_label[predictionsMade[i]])\n",
    "\n",
    "from IPython.display import display\n",
    "input_text = widgets.Text(\n",
    "    value=\"predicted \" + out2.widget.result,\n",
    "    placeholder='Type label here',\n",
    "    disabled=False\n",
    ")\n",
    "output_text = widgets.Text(\n",
    "    value=\"predicted \" + out2.widget.result,\n",
    "    placeholder='New Header will be displayed here',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "def bind_input_to_output(sender):\n",
    "    output_text.value = input_text.value\n",
    "\n",
    "input_text.observe(bind_input_to_output)\n",
    "\n",
    "print(\"Input new column Header Label: \")\n",
    "\n",
    "display(input_text)\n",
    "display(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save the new version of CSV file, and give a name to new survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the new column w/ it's new column name\n",
    "pred_conf_col = \"pred_conf \" + input_text.value\n",
    "test_train_col = \"test_train \" + input_text.value\n",
    "\n",
    "df[input_text.value] = finalPred\n",
    "df[pred_conf_col] = prediction_confidence\n",
    "df[test_train_col] = test_train_list\n",
    "\n",
    "print(input_text.value)\n",
    "\n",
    "new_file = absolutePath + csv_file[:-4]+'_v1.csv'\n",
    "printmd(\"<b><span style='color:red'>A new temporary file will be created at: </span></b>\")\n",
    "print(new_file)\n",
    "df.to_csv(new_file, index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input survey name\n",
    "\n",
    "default_name = csv_file.split(\".\")[0] + \"_\" + str(EPOCHS) + \"_\" + str(BS) + \"_\" + str(im_dimension)\n",
    "\n",
    "from IPython.display import display\n",
    "input_text = widgets.Text(value=default_name)\n",
    "output_text = widgets.Text(value=default_name)\n",
    "\n",
    "def bind_input_to_output(sender):\n",
    "    output_text.value = input_text.value\n",
    "\n",
    "# Tell the text input widget to call bind_input_to_output() on submit\n",
    "input_text.on_submit(bind_input_to_output)\n",
    "\n",
    "printmd(\"<b><span style='color:red'>Input survey name here, press Enter, and then run the next cell:</span></b>\")\n",
    "# Display input text box widget for input\n",
    "display(input_text)\n",
    "\n",
    "display(output_text)\n",
    "\n",
    "survey_name = output_text.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print survey name\n",
    "survey_name = output_text.value\n",
    "printmd(\"<b><span style='color:red'>Survey Name is: </span></b>\" + survey_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate the survey and create survey URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "referer = survey_url.split(\"/main\")[0] +\"/\"\n",
    "upload_url = referer + \"uploadCSV\"\n",
    "new_survey_url_base = survey_url.split(user)[0]\n",
    "\n",
    "import requests\n",
    "import re\n",
    "csv = {\"file\": open(new_file, \"rb\")}\n",
    "upload_data = {\n",
    "    'name': input_text.value,\n",
    "    'dzc': dzc_file,\n",
    "    'user':user\n",
    "}\n",
    "headers = {\n",
    "    'User-Agent': 'suave user agent',\n",
    "    'referer': referer\n",
    "}\n",
    "\n",
    "r = requests.post(upload_url, files=csv, data=upload_data, headers=headers)\n",
    "\n",
    "if r.status_code == 200:\n",
    "    printmd(\"<b><span style='color:red'>New survey created successfully</span></b>\")\n",
    "    regex = re.compile('[^0-9a-zA-Z_]')\n",
    "    s_url = survey_name\n",
    "    s_url =  regex.sub('_', s_url)\n",
    "\n",
    "    url = new_survey_url_base + user + \"_\" + s_url + \".csv\" + \"&views=\" + views + \"&view=\" + view\n",
    "    print(url)\n",
    "    printmd(\"<b><span style='color:red'>Click the URL to open the new survey</span></b>\")\n",
    "else:\n",
    "    printmd(\"<b><span style='color:red'>Error creating new survey. Check if a survey with this name already exists.</span></b>\")\n",
    "    printmd(\"<b><span style='color:red'>Reason: </span></b>\"+ str(r.status_code) + \" \" + r.reason)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
